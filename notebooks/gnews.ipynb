{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\claudion\\OneDrive - Cielo\\02. Maritalk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "# Obtendo o diretório atual do notebook\n",
    "diretorio_atual = os.getcwd()\n",
    "\n",
    "# Definindo o diretório raiz como \"arquivos\"\n",
    "diretorio_raiz = r\"c:\\Users\\claudion\\OneDrive - Cielo\\02. Maritalk\"\n",
    "\n",
    "# Mudando o diretório de trabalho para a pasta raiz\n",
    "os.chdir(diretorio_raiz)\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar função para retornar as noticias mais recentes de uma query\n",
    "from GoogleNews import GoogleNews\n",
    "\n",
    "def get_news(query):\n",
    "    googlenews = GoogleNews()\n",
    "    googlenews.setlang('pt')\n",
    "    googlenews.setperiod('7d')\n",
    "    googlenews.search(query)\n",
    "    result = googlenews.result()\n",
    "    df = pd.DataFrame(result)\n",
    "    return df\n",
    "\n",
    "df = get_news('RS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'media', 'date', 'datetime', 'desc', 'link', 'img'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://agenciabrasil.ebc.com.br/geral/noticia/2024-05/quinze-quilombos-estao-totalmente-isolados-pelas-chuvas-no-rs',\n",
       " 'https://g1.globo.com/meio-ambiente/noticia/2024/05/16/apos-passagem-de-frente-fria-chuva-volta-ao-rs-nesta-quinta-volumes-nao-devem-superar-100-milimetros.ghtml',\n",
       " 'https://g1.globo.com/politica/noticia/2024/05/15/lula-anuncia-medidas-para-familias-atingidas-pelas-enxurradas-no-rs.ghtml',\n",
       " 'https://jovempan.com.br/noticias/brasil/quase-12-mil-animais-ja-foram-resgatados-das-enchentes-no-rs.html']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests # Required to make HTTP requests\n",
    "from bs4 import BeautifulSoup # Required to parse HTML\n",
    "import numpy as np # Required to dedupe sites\n",
    "from urllib.parse import unquote # Required to unquote URLs\n",
    "query = \"chuvas RS\"\n",
    "response = requests.get(f\"https://www.google.com/search?q={query}&tbm=nws\") # Make the request\n",
    "soup = BeautifulSoup(response.text, \"html.parser\") # Parse the HTML\n",
    "links = soup.find_all(\"a\") # Find all the links in the HTML\n",
    "\n",
    "# loop over `links` and keep only the one that have the href starting with \"/url?q=\"\n",
    "urls = []\n",
    "for l in [link for link in links if link[\"href\"].startswith(\"/url?q=\")]:\n",
    "    # get the url\n",
    "    url = l[\"href\"]\n",
    "    # remove the \"/url?q=\" part\n",
    "    url = url.replace(\"/url?q=\", \"\")\n",
    "    # remove the part after the \"&sa=...\"\n",
    "    url = unquote(url.split(\"&sa=\")[0])\n",
    "    # special case for google scholar\n",
    "    if url.startswith(\"https://scholar.google.com/scholar_url?url=http\"):\n",
    "        url = url.replace(\"https://scholar.google.com/scholar_url?url=\", \"\").split(\"&\")[0]\n",
    "    elif 'google.com/' in url: # skip google links\n",
    "        continue\n",
    "    if url.endswith('.pdf'): # skip pdf links\n",
    "        continue\n",
    "    if '#' in url: # remove anchors (e.g. wikipedia.com/bob#history and wikipedia.com/bob#genetics are the same page)\n",
    "        url = url.split('#')[0]\n",
    "    # print the url\n",
    "    urls.append(url)\n",
    "\n",
    "# Use numpy to dedupe the list of urls after removing anchors\n",
    "urls = list(np.unique(urls))[1:5]\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readabilipy import simple_json_from_html_string\n",
    "from langchain.schema import Document # Required to create a Document object\n",
    "import requests\n",
    "def scrape_and_parse(url: str) -> Document:\n",
    "    \"\"\"Scrape a webpage and parse it into a Document object\"\"\"\n",
    "    req = requests.get(url)\n",
    "    article = simple_json_from_html_string(req.text, use_readability=True)\n",
    "    # The following line seems to work with the package versions on my local machine, but not on Google Colab\n",
    "    # return Document(page_content=article['plain_text'][0]['text'], metadata={'source': url, 'page_title': article['title']})\n",
    "    return Document(page_content='\\n\\n'.join([a['text'] for a in article['plain_text']]), metadata={'source': url, 'page_title': article['title']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: node executable not found, reverting to pure-Python mode. Install Node.js v10 or newer to use Readability.js.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Quase 12 mil animais já foram resgatados das enchentes no RS | Jovem Pan\\n\\nMinha conta\\n\\nJovem Pan\\n\\n> Notícias\\n\\n> Brasil\\n\\n> Quase 12 mil animais já foram resgatados das enchentes no RS\\n\\nQuase 12 mil animais já foram resgatados das enchentes no RS\\n\\nONGs e voluntários estão na linha de frente desde o início das chuvas para salvar os bichos\\n\\n* Por Tamyres Sbrile, * 16/05/2024 20h08,\\n\\nWILTON JUNIOR/ESTADÃO CONTEÚDOEstado tem enfrentado chuvas fortes desde o fim de abril\\n\\nSegundo último boletim da Defesa Civil do Rio Grande do Sul, cerca de 11.932 animais foram resgatados das enchentes que atingem o estado. Cães, gatos, porcos, cavalos, como foi o caso do “Caramelo”, e aves estão sendo resgatados diariamente por ONGs e bombeiros civis. Segundo a secretaria estadual do Meio Ambiente e Infraestrutura (Sema), a maioria dos bichos resgatados são cães e gatos, mas animais silvestres também estão sendo socorridos, como guaxinins. Ainda conforme a secretaria, os animais que são recolhidos passam por uma triagem veterinária, para separar os que estão com problemas de saúde e os que estão aptos a retornarem aos tutores ou serem levados para abrigos. Os bichos estão sendo enviados para abrigos públicos, ONGs ou clínicas veterinárias, dependendo do estado de saúde.\\n\\nSiga o canal da Jovem Pan News e receba as principais notícias no seu WhatsApp!\\n\\nWhatsApp\\n\\nO grupo Carrefour Brasil, cedeu à prefeitura de São Leopoldo, um galpão que anteriormente funcionava um hipermercado, mas como o espaço está em desuso, foi improvisado como abrigo para os animais que chegam das enchentes.\\n\\nNas redes sociais, vários perfis que estão na linha de frente no sul, compartilham a rotina de resgate e captura dos animais, incentivando as doações e até mesmo ajudando os tutores a encontrarem os pets que estão perdidos.\\n\\nA ONG Campo Bom Pra Cachorro, da cidade de Campo Bom, é um dos exemplos de quem está desde o início das chuvas atuando de forma voluntária no resgate dos animais. Eles criaram, inclusive, um perfil nas redes para postar fotos dos animais que já foram resgatados e estão prontos para retornar aos donos. Uma das cachorrinhas que foi encontrada, ficou cerca de 12 dias sem se alimentar e acabou ingerindo pedra por fome. Ela precisou passar por uma cirurgia para retirada das pedras do estômago e agora esta se recuperando aos poucos.\\n\\nPara ajudar a ONG doe por Pix no CNPJ: 24494672000169 ou ongcampobompracachorro@gmail.com.\\n\\nVer essa foto no Instagram\\n\\nUma publicação compartilhada por Kayanne Braga (@kayannebraga)\\n\\nLeia também\\n\\nCorreios pedem que cessem doações de roupas ao RS\\n\\nGisele Bündchen arrecada R$ 4,5 milhões para vítimas de enchentes no RS\\n\\n* Tags:, * animais, Chuvas, Enchentes, resgatados, Rio Grande do Sul,\\n\\nComentários\\n\\nConteúdo para assinantes. Assine JP Premium.\\n\\nTorne-se um assinante >\\n\\n* Mais JPÚltimas NotíciasOpinião Jovem PanVideos, * PodcastsProgramaçãoJP News, * Ouça a Rádio Jovem Pan Ao VivoMitos e FatosBlogs, * Nossa RedaçãoAfiliadasSobre a Jovem Pan,\\n\\n* Anuncie, * Feed RSS, * Aplicativos, * Política de Privacidade, * Nossa Redação,\\n\\n* (11) 93117-0620,\\n\\nTodos os direitos reservados - Portal Jovem Pan Online - Rádio Panamericana S/A Av. Paulista, 807 - 24o andar - Cerqueira César - São Paulo - SP\\n\\n+55 11 2870-9700 - jovempanonline@jovempan.com.br'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcs = scrape_and_parse(urls[-1])\n",
    "dcs.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A notícia relata o esforço de ONGs e voluntários no resgate de animais afetados pelas enchentes no Rio Grande do Sul. Até a data do último boletim da Defesa Civil, cerca de 11.932 animais, incluindo cães, gatos, porcos, cavalos e aves, foram salvos. A secretaria estadual do Meio Ambiente e Infraestrutura (Sema) informa que a maioria dos animais resgatados são cães e gatos, mas também há animais silvestres, como guaxinins. Os animais passam por uma triagem veterinária e são encaminhados para abrigos ou clínicas, dependendo de sua condição de saúde.\n",
      "\n",
      "Além disso, o grupo Carrefour Brasil cedeu um galpão em São Leopoldo para servir como abrigo temporário para os animais resgatados. Nas redes sociais, diversos perfis compartilham a rotina dos resgates e incentivam doações e ações para encontrar animais perdidos.\n",
      "\n",
      "A ONG Campo Bom Pra Cachorro, de Campo Bom, é um exemplo de organização que está atuando no resgate dos animais desde o início das chuvas. Eles criaram um perfil nas redes sociais para postar fotos dos animais resgatados e prontos para serem devolvidos aos donos. Uma das cachorrinhas resgatadas, que ficou sem se alimentar por cerca de 12 dias e ingeriu pedras por fome, precisou passar por uma cirurgia para retirada das pedras do estômago e agora está se recuperando.\n",
      "\n",
      "Para ajudar a ONG, doações podem ser feitas por Pix com o CNPJ: 24.494.672/0001-69 ou pelo e-mail ongcampobompracachorro@gmail.com.\n"
     ]
    }
   ],
   "source": [
    "from utils.llm import *\n",
    "\n",
    "apikey = read_key()\n",
    "prompt = f\"Faça um resumo da seguinte notícia: {dcs.page_content}\"\n",
    "retorno = get_response(prompt, apikey)\n",
    "print(retorno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\claudion\\OneDrive - Cielo\\02. Maritalk\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.cnnbrasil.com.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\claudion\\OneDrive - Cielo\\02. Maritalk\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.cnnbrasil.com.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\claudion\\OneDrive - Cielo\\02. Maritalk\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.cnnbrasil.com.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\claudion\\OneDrive - Cielo\\02. Maritalk\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'revistaforum.com.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\claudion\\OneDrive - Cielo\\02. Maritalk\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.poder360.com.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\claudion\\OneDrive - Cielo\\02. Maritalk\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.cnnbrasil.com.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\claudion\\OneDrive - Cielo\\02. Maritalk\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.cnnbrasil.com.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\claudion\\OneDrive - Cielo\\02. Maritalk\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.cnnbrasil.com.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\claudion\\OneDrive - Cielo\\02. Maritalk\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'oantagonista.com.br'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\claudion\\AppData\\Local\\Temp\\7\\ipykernel_25208\\1244432249.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['link'].apply(get_text)\n"
     ]
    }
   ],
   "source": [
    "#Para cada link, acessar o site e extrair o texto da notícia\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_text(link):\n",
    "    try:\n",
    "        page = requests.get(link)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        text = soup.find_all('a')\n",
    "        for link in text:\n",
    "            href = link.get('href')\n",
    "        return text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['text'] = df['link'].apply(get_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p><a href=\"https://go.microsoft.com/fwlink/?LinkID=62293&amp;IIS70Error=404,0,0x80070002,17763\">View more information »</a></p>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://ealexbarros.medium.com/utilizando-python-para-armazenar-not%C3%ADcias-google-news-api-6d37f84cd426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Completo END</th>\n",
       "      <th>Código SAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AV JOSE RAIMUNDO,3949  -GRANJAS VAGALUME ,IPAT...</td>\n",
       "      <td>5832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AV. ANEL DO CONTORNO,196  -ESPÍRITO SANTO ,VIT...</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R PROF DARTAGNAN ALVES,48  -CENTRO ,ARCOVERDE, PE</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R JOAQUIM TAVORA,244  -SAO FRANCISCO ,CARUARU, PE</td>\n",
       "      <td>5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R FELIPE CAMARAO,235  QUADRA AREA LOTE 02JARDI...</td>\n",
       "      <td>9916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>R FENANDO DE NORONHA,4176  -NOVA FLORESTA ,POR...</td>\n",
       "      <td>9867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>R JOANA BATISTA HOLANDA,135  ASALGADINHO ,JUAZ...</td>\n",
       "      <td>4152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>R FLODOALDO RODRIGUES,665  JARDIM AUREA ,VARGI...</td>\n",
       "      <td>5430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>R SEBASTIANA MARIA DE JESUS,317  -NOSSA SENHOR...</td>\n",
       "      <td>7221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>R LAZARO TADEU PEREIRA,25  -ALVORADA ,PATOS DE...</td>\n",
       "      <td>4255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Completo END  Código SAP\n",
       "0   AV JOSE RAIMUNDO,3949  -GRANJAS VAGALUME ,IPAT...        5832\n",
       "1   AV. ANEL DO CONTORNO,196  -ESPÍRITO SANTO ,VIT...        4500\n",
       "2   R PROF DARTAGNAN ALVES,48  -CENTRO ,ARCOVERDE, PE        5650\n",
       "3   R JOAQUIM TAVORA,244  -SAO FRANCISCO ,CARUARU, PE        5500\n",
       "4   R FELIPE CAMARAO,235  QUADRA AREA LOTE 02JARDI...        9916\n",
       "..                                                ...         ...\n",
       "68  R FENANDO DE NORONHA,4176  -NOVA FLORESTA ,POR...        9867\n",
       "69  R JOANA BATISTA HOLANDA,135  ASALGADINHO ,JUAZ...        4152\n",
       "70  R FLODOALDO RODRIGUES,665  JARDIM AUREA ,VARGI...        5430\n",
       "71  R SEBASTIANA MARIA DE JESUS,317  -NOSSA SENHOR...        7221\n",
       "72  R LAZARO TADEU PEREIRA,25  -ALVORADA ,PATOS DE...        4255\n",
       "\n",
       "[73 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enderecos = pd.read_excel(r\"C:\\Users\\claudion\\Downloads\\FDEX_AUSENTES.xlsx\")\n",
    "enderecos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "latitudes = []\n",
    "longitudes = []\n",
    "\n",
    "for i in range(len(enderecos)):\n",
    "    countryRegion = 'BR'\n",
    "    # addressLine = \"AV. ANEL DO CONTORNO,196  -ESPÍRITO SANTO ,VITORIA DA CONQUISTA, BA\"\n",
    "    addressLine = enderecos['Completo END'][i]\n",
    "    maxResults = 1\n",
    "    BingMapsKey = 'AlCTh-7wnjGDButsudsD62gzEhUfWYVzEowrj4E_XRTGWCM8GpWCjw7En119U76a'\n",
    "    url = f\"http://dev.virtualearth.net/REST/v1/Locations?countryRegion={countryRegion}&addressLine={addressLine}&maxResults={maxResults}&key={BingMapsKey}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    status_code = response.status_code\n",
    "\n",
    "    if status_code != 200:\n",
    "        print(\"Erro na requisição\")\n",
    "        print('\\n')\n",
    "        print(i)\n",
    "        latitude = np.nan\n",
    "        longitude = np.nan\n",
    "\n",
    "    else:\n",
    "        latitude = data['resourceSets'][0]['resources'][0]['geocodePoints'][0]['coordinates'][0]\n",
    "        longitude = data['resourceSets'][0]['resources'][0]['geocodePoints'][0]['coordinates'][1]\n",
    "    \n",
    "    latitudes.append(latitude)\n",
    "    longitudes.append(longitude)\n",
    "\n",
    "enderecos['LAT'] = latitudes\n",
    "enderecos['LONG'] = longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "enderecos.to_excel(r\"C:\\Users\\claudion\\Downloads\\FDEX_AUSENTES.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
